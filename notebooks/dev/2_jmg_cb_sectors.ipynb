{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrunchBase sector modelling\n",
    "\n",
    "We want to consolidate CrunchBase's sectors into a smaller set for reporting. We also want to know what is the most important sector for each company, for which we will implement a machine learning analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy\n",
    "\n",
    "%run ../utilities.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_getters.core import get_engine\n",
    "from data_getters.inspector import get_schemas\n",
    "from random import sample\n",
    "from itertools import combinations, product, chain\n",
    "import networkx as nx\n",
    "import community\n",
    "\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../lda_pipeline.py\n",
    "from gensim import corpora, models\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Characters to drop\n",
    "drop_characters = re.sub('-','',punctuation)+digits\n",
    "\n",
    "#Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('English')\n",
    "\n",
    "#Stem functions\n",
    "from nltk.stem import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def clean_tokenise(string,drop_characters=drop_characters,stopwords=stop):\n",
    "    '''\n",
    "    Takes a string and cleans (makes lowercase and removes stopwords)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "\n",
    "    #Lowercase\n",
    "    str_low = string.lower()\n",
    "    \n",
    "    \n",
    "    #Remove symbols and numbers\n",
    "    str_letters = re.sub('[{drop}]'.format(drop=drop_characters),'',str_low)\n",
    "    \n",
    "    \n",
    "    #Remove stopwords\n",
    "    clean = [x for x in str_letters.split(' ') if (x not in stop) & (x!='')]\n",
    "    \n",
    "    return(clean)\n",
    "\n",
    "\n",
    "class CleanTokenize():\n",
    "    '''\n",
    "    This class takes a list of strings and returns a tokenised, clean list of token lists ready\n",
    "    to be processed with the LdaPipeline\n",
    "    \n",
    "    It has a clean method to remove symbols and stopwords\n",
    "    \n",
    "    It has a bigram method to detect collocated words\n",
    "    \n",
    "    It has a stem method to stem words\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,corpus):\n",
    "        '''\n",
    "        Takes a corpus (list where each element is a string)\n",
    "        '''\n",
    "        \n",
    "        #Store\n",
    "        self.corpus = corpus\n",
    "        \n",
    "    def clean(self,drop=drop_characters,stopwords=stop):\n",
    "        '''\n",
    "        Removes strings and stopwords, \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        cleaned = [clean_tokenise(doc,drop_characters=drop,stopwords=stop) for doc in self.corpus]\n",
    "        \n",
    "        self.tokenised = cleaned\n",
    "        return(self)\n",
    "    \n",
    "    def stem(self):\n",
    "        '''\n",
    "        Optional: stems words\n",
    "        \n",
    "        '''\n",
    "        #Stems each word in each tokenised sentence\n",
    "        stemmed = [[stemmer.stem(word) for word in sentence] for sentence in self.tokenised]\n",
    "    \n",
    "        self.tokenised = stemmed\n",
    "        return(self)\n",
    "        \n",
    "    \n",
    "    def bigram(self,threshold=10):\n",
    "        '''\n",
    "        Optional Create bigrams.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Colocation detector trained on the data\n",
    "        phrases = models.Phrases(self.tokenised,threshold=threshold)\n",
    "        \n",
    "        bigram = models.phrases.Phraser(phrases)\n",
    "        \n",
    "        self.tokenised = bigram[self.tokenised]\n",
    "        \n",
    "        return(self)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "class LdaPipeline():\n",
    "    '''\n",
    "    This class processes lists of keywords.\n",
    "    How does it work?\n",
    "    -It is initialised with a list where every element is a collection of keywords\n",
    "    -It has a method to filter keywords removing those that appear less than a set number of times\n",
    "    \n",
    "    -It has a method to process the filtered df into an object that gensim can work with\n",
    "    -It has a method to train the LDA model with the right parameters\n",
    "    -It has a method to predict the topics in a corpus\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,corpus):\n",
    "        '''\n",
    "        Takes the list of terms\n",
    "        '''\n",
    "        \n",
    "        #Store the corpus\n",
    "        self.tokenised = corpus\n",
    "        \n",
    "    def filter(self,minimum=5):\n",
    "        '''\n",
    "        Removes keywords that appear less than 5 times.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load\n",
    "        tokenised = self.tokenised\n",
    "        \n",
    "        #Count tokens\n",
    "        token_counts = pd.Series([x for el in tokenised for x in el]).value_counts()\n",
    "        \n",
    "        #Tokens to keep\n",
    "        keep = token_counts.index[token_counts>minimum]\n",
    "        \n",
    "        #Filter\n",
    "        tokenised_filtered = [[x for x in el if x in keep] for el in tokenised]\n",
    "        \n",
    "        #Store\n",
    "        self.tokenised = tokenised_filtered\n",
    "        self.empty_groups = np.sum([len(x)==0 for x in tokenised_filtered])\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def clean(self):\n",
    "        '''\n",
    "        Remove symbols and numbers\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    def process(self):\n",
    "        '''\n",
    "        This creates the bag of words we use in the gensim analysis\n",
    "        \n",
    "        '''\n",
    "        #Load the list of keywords\n",
    "        tokenised = self.tokenised\n",
    "        \n",
    "        #Create the dictionary\n",
    "        dictionary = corpora.Dictionary(tokenised)\n",
    "        \n",
    "        #Create the Bag of words. This converts keywords into ids\n",
    "        corpus = [dictionary.doc2bow(x) for x in tokenised]\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.dictionary = dictionary\n",
    "        return(self)\n",
    "        \n",
    "    def tfidf(self):\n",
    "        '''\n",
    "        This is optional: We extract the term-frequency inverse document frequency of the words in\n",
    "        the corpus. The idea is to identify those keywords that are more salient in a document by normalising over\n",
    "        their frequency in the whole corpus\n",
    "        \n",
    "        '''\n",
    "        #Load the corpus\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Fit a TFIDF model on the data\n",
    "        tfidf = models.TfidfModel(corpus)\n",
    "        \n",
    "        #Transform the corpus and save it\n",
    "        self.corpus = tfidf[corpus]\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def fit_lda(self,num_topics=20,passes=5,iterations=75,random_state=1803):\n",
    "        '''\n",
    "        \n",
    "        This fits the LDA model taking a set of keyword arguments.\n",
    "        #Number of passes, iterations and random state for reproducibility. We will have to consider\n",
    "        reproducibility eventually.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load the corpus\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Train the LDA model with the parameters we supplied\n",
    "        lda = models.LdaModel(corpus,id2word=self.dictionary,\n",
    "                              num_topics=num_topics,passes=passes,iterations=iterations,random_state=random_state)\n",
    "        \n",
    "        #Save the outputs\n",
    "        self.lda_model = lda\n",
    "        self.lda_topics = lda.show_topics(num_topics=num_topics)\n",
    "        \n",
    "\n",
    "        return(self)\n",
    "    \n",
    "    def predict_topics(self):\n",
    "        '''\n",
    "        This predicts the topic mix for every observation in the corpus\n",
    "        \n",
    "        '''\n",
    "        #Load the attributes we will be working with\n",
    "        lda = self.lda_model\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Now we create a df\n",
    "        predicted = lda[corpus]\n",
    "        \n",
    "        #Convert this into a dataframe\n",
    "        predicted_df = pd.concat([pd.DataFrame({x[0]:x[1] for x in topics},\n",
    "                                              index=[num]) for num,topics in enumerate(predicted)]).fillna(0)\n",
    "        \n",
    "        self.predicted_df = predicted_df\n",
    "        \n",
    "        return(self)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../text_classifier.py\n",
    "# CLasses\n",
    "\n",
    "#ML imports\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score,GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore',UserWarning)\n",
    "\n",
    "#One class for text classification based on text inputs\n",
    "\n",
    "class TextClassification():\n",
    "    '''\n",
    "    This class takes a corpus (could be a list of strings or a tokenised corpus) and a target (could be multiclass or single class).\n",
    "    \n",
    "    When it is initialised it vectorises the list of tokens using sklearn's count vectoriser.\n",
    "    \n",
    "    It has a grid search method that takes a list of models and parameters and trains the model.\n",
    "    \n",
    "    It returns the output of grid search for diagnosis\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,corpus,target):\n",
    "        '''\n",
    "        \n",
    "        Initialise. The class will recognise if we are feeding it a list of strings or a list of\n",
    "        tokenised documents and vectorise accordingly. \n",
    "        \n",
    "        It will also recognise is this a multiclass or one class problem based on the dimensions of the target array\n",
    "        \n",
    "        Later on, it will use control flow to modify model parameters depending on the type of data we have\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Is this a multiclass classification problem or a single class classification problem?\n",
    "        if target.shape[1]>1:\n",
    "            self.mode = 'multiclass'\n",
    "            \n",
    "        else:\n",
    "            self.mode = 'single_class'\n",
    "    \n",
    "    \n",
    "        #Store the target\n",
    "        self.Y = target\n",
    "    \n",
    "        #Did we feed the model a bunch of strings or a list of tokenised docs? If the latter, we clean and tokenise.\n",
    "        \n",
    "        if type(corpus[0])==str:\n",
    "            corpus = CleanTokenize(corpus).clean().bigram().tokenised\n",
    "            \n",
    "        #Turn every list of tokens into a string for count vectorising\n",
    "        corpus_string =  [' '.join(words) for words in corpus]\n",
    "        \n",
    "        \n",
    "        #And then we count vectorise in a hacky way.\n",
    "        count_vect = CountVectorizer(stop_words='english',min_df=5).fit(corpus_string)\n",
    "        \n",
    "        #Store the features\n",
    "        self.X = count_vect.transform(corpus_string)\n",
    "        \n",
    "        #Store the count vectoriser (we will use it later on for prediction on new data)\n",
    "        self.count_vect = count_vect\n",
    "        \n",
    "    def grid_search(self,models):\n",
    "        '''\n",
    "        The grid search method takes a list with models and their parameters and it does grid search crossvalidation.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load inputs and targets into the model\n",
    "        Y = self.Y\n",
    "        X = self.X\n",
    "        \n",
    "        if self.mode=='multiclass':\n",
    "            '''\n",
    "            If the model is multiclass then we need to add some prefixes to the model paramas\n",
    "            \n",
    "            '''\n",
    "        \n",
    "            for mod in models:\n",
    "                #Make ovr\n",
    "                mod[0] = OneVsRestClassifier(mod[0])\n",
    "                \n",
    "                #Add the estimator prefix\n",
    "                mod[1] = {'estimator__'+k:v for k,v in mod[1].items()}\n",
    "                \n",
    "        \n",
    "        #Container with results\n",
    "        results = []\n",
    "\n",
    "        #For each model, run the analysis.\n",
    "        for num,mod in enumerate(models):\n",
    "            print(num)\n",
    "\n",
    "            #Run the classifier\n",
    "            clf = GridSearchCV(mod[0],mod[1])\n",
    "\n",
    "            #Fit\n",
    "            clf.fit(X,Y)\n",
    "\n",
    "            #Append results\n",
    "            results.append(clf)\n",
    "        \n",
    "        self.results = results\n",
    "        return(self)\n",
    "\n",
    "    \n",
    "#Class to visualise the outputs of multilabel models.\n",
    "\n",
    "#I call it OrangeBrick after YellowBrick, the package for ML output visualisation \n",
    "#(which currently doesn't support multilabel classification)\n",
    "\n",
    "\n",
    "class OrangeBrick():\n",
    "    '''\n",
    "    This class takes a df with the true classes for a multilabel classification exercise and produces some charts visualising findings.\n",
    "    \n",
    "    The methods include:\n",
    "    \n",
    "        .confusion_stack: creates a stacked barchart with the confusion matrices stacked by category, sorting classes by performance\n",
    "        .prec_rec: creates a barchart showing each class precision and recall;\n",
    "        #Tobe done: Consider mixes between classes?\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,true_labels,predicted_labels,var_names):\n",
    "        '''\n",
    "        Initialise with a true labels, predicted labels and the variable names\n",
    "        '''\n",
    "         \n",
    "        self.true_labels = true_labels\n",
    "        self.predicted_labels = predicted_labels\n",
    "        self.var_names = var_names\n",
    "    \n",
    "    def make_metrics(self):\n",
    "        '''\n",
    "        Estimates performance metrics (for now just confusion charts by class and precision/recall scores for the 0.5 \n",
    "        decision rule.\n",
    "        \n",
    "        '''\n",
    "        #NB in a confusion matrix in SKlearn the X axis indicates the predicted class and the Y axis indicates the ground truth.\n",
    "        #This means that:\n",
    "            #cf[0,0]-> TN\n",
    "            #cf[1,1]-> TP\n",
    "            #cf[0,1]-> FN (prediction is false, groundtruth is true)\n",
    "            #cf[1,0]-> FP (prediction is true, ground truth is false)\n",
    "\n",
    "\n",
    "\n",
    "        #Predictions and true labels\n",
    "        true_labels = self.true_labels\n",
    "        pred_labels = self.predicted_labels\n",
    "\n",
    "        #Variable names\n",
    "        var_names = self.var_names\n",
    "\n",
    "        #Store confusion matrices\n",
    "        score_store = []\n",
    "\n",
    "\n",
    "        for num in np.arange(len(var_names)):\n",
    "\n",
    "            #This is the confusion matrix\n",
    "            cf = confusion_matrix(pred_labels[:,num],true_labels[:,num])\n",
    "\n",
    "            #This is a melted confusion matrix\n",
    "            melt_cf = pd.melt(pd.DataFrame(cf).reset_index(drop=False),id_vars='index')['value']\n",
    "            melt_cf.index = ['true_negative','false_positive','false_negative','true_positive']\n",
    "            melt_cf.name = var_names[num]\n",
    "            \n",
    "            #Order variables to separate failed vs correct predictions\n",
    "            melt_cf = melt_cf.loc[['true_positive','true_negative','false_positive','false_negative']]\n",
    "\n",
    "            #We are also interested in precision and recall\n",
    "            prec = cf[1,1]/(cf[1,1]+cf[1,0])\n",
    "            rec = cf[1,1]/(cf[1,1]+cf[0,1])\n",
    "\n",
    "            prec_rec = pd.Series([prec,rec],index=['precision','recall'])\n",
    "            prec_rec.name = var_names[num]\n",
    "            score_store.append([melt_cf,prec_rec])\n",
    "    \n",
    "        self.score_store = score_store\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def confusion_chart(self,ax):\n",
    "        '''\n",
    "        Plot the confusion charts\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Visualise confusion matrix outputs\n",
    "        cf_df = pd.concat([x[0] for x in self.score_store],1)\n",
    "\n",
    "        #This ranks categories by the error rates\n",
    "        failure_rate = cf_df.apply(lambda x: x/x.sum(),axis=0).loc[['false' in x for x in cf_df.index]].sum().sort_values(\n",
    "            ascending=False).index\n",
    "\n",
    "        \n",
    "        #Plot and add labels\n",
    "        cf_df.T.loc[failure_rate,:].plot.bar(stacked=True,ax=ax,width=0.8,cmap='Accent')\n",
    "\n",
    "        ax.legend(bbox_to_anchor=(1.01,1))\n",
    "        #ax.set_title('Stacked confusion matrix for disease areas',size=16)\n",
    "    \n",
    "    \n",
    "    def prec_rec_chart(self,ax):\n",
    "        '''\n",
    "        \n",
    "        Plot a precision-recall chart\n",
    "        \n",
    "        '''\n",
    "    \n",
    "\n",
    "        #Again, we sort them here to assess model performance in different disease areas\n",
    "        prec_rec = pd.concat([x[1] for x in self.score_store],1).T.sort_values('precision')\n",
    "        prec_rec.plot.bar(ax=ax)\n",
    "\n",
    "        #Add legend and title\n",
    "        ax.legend(bbox_to_anchor=(1.01,1))\n",
    "        #ax.set_title('Precision and Recall by disease area',size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create this type to deal with some Nones later\n",
    "NoneType = type(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We parse the category lists\n",
    "cb = pd.read_csv('../../data/external/2019-09-10_cb_companies.csv',compression='zip',converters={'category_name':literal_eval})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here the idea is to create a proximity matrix based on co-occurrences\n",
    "\n",
    "#Turn co-occurrences into combinations of pairs we can use to construct a similarity matrix\n",
    "sector_combs = flatten_list([sorted(list(combinations(x,2))) for x in cb['category_name']])\n",
    "sector_combs = [x for x in sector_combs if len(x)>0]\n",
    "\n",
    "#Turn the sector combs into an edgelist\n",
    "edge_list = pd.DataFrame(sector_combs,columns=['source','target'])\n",
    "\n",
    "edge_list['weight']=1\n",
    "\n",
    "#Group over edge pairs to aggregate weights\n",
    "edge_list_weighted = edge_list.groupby(['source','target'])['weight'].sum().reset_index(drop=False)\n",
    "\n",
    "edge_list_weighted.sort_values('weight',ascending=False).head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create network and extract communities\n",
    "net = nx.from_pandas_edgelist(edge_list_weighted,edge_attr=True)\n",
    "\n",
    "#We choose a high level of resulution (lower == more finely grained)\n",
    "#comms = community.best_partition(net,resolution=0.2)\n",
    "\n",
    "# with open(f'../../data/aux/{today_str}_communities.json','w') as outfile:\n",
    "#     json.dump(comms,outfile)\n",
    "\n",
    "#We have chosen quite a finely grained level here in order to obtain categories that we can label as creative.\n",
    "\n",
    "with open(f'../../data/aux/10_9_2019_communities.json','r') as infile:\n",
    "    comms = json.load(infile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #What does this look like?\n",
    "# comm_strings = pd.DataFrame(comms,index=['comm']).T.groupby('comm')\n",
    "\n",
    "# #This is just to visualise the participation in communities\n",
    "# for n,x in enumerate(comm_strings.groups.keys()):\n",
    "#     print(n)\n",
    "#     print('====')\n",
    "#     print('\\t'.join(list(comm_strings.groups[x])))\n",
    "#     #print(', '.join(list(x.index())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the CB categories and create a lookup\n",
    "with open('../../data/aux/industrial_manufacturing,.txt','r') as infile:\n",
    "    cb_cats = infile.read().split('\\n')\n",
    "    \n",
    "cb_cats_lookup = {n:x for n,x in enumerate(cb_cats)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_cats_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lookup every category\n",
    "cb['sector_list']= [[cb_cats_lookup[comms[x]] for x in cats] for cats in cb['category_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify one-sector categories\n",
    "cb['sector_unique'] = [list(set(x))[0] if len(set(x))==1 else 'mixed' for x in cb['sector_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looks like enough for a model\n",
    "cb['sector_unique'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is the corpus. We drop projects without any org labels, and projects without abstracts\n",
    "\n",
    "#We focus on 'pure cases'\n",
    "cb_pure_cases = cb.loc[[x !='mixed' for x in cb['sector_unique']]].reset_index(drop=True)\n",
    "\n",
    "#Drop cases with no descriptions\n",
    "cb_pure_cases = cb_pure_cases.dropna(axis=0,subset=['long_description']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(cb_pure_cases['long_description'])\n",
    "\n",
    "#We use a utility function to create a df for a one vs rest classification\n",
    "target = pd.get_dummies(cb_pure_cases['sector_unique'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run grid search with these model parameters\n",
    "my_models = [\n",
    "#    [RandomForestClassifier(),\n",
    "#     {'class_weight':['balanced'],'min_samples_leaf':[1,5]}],\n",
    "    \n",
    "    [LogisticRegression(),\n",
    "     {'class_weight':['balanced'],'penalty':['l1','l2'],\n",
    "      'C':[0.1,1,100]}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict groups\n",
    "\n",
    "#Initialise the TextClassification class\n",
    "cb_t = TextClassification(corpus,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_t.grid_search(my_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check scores and best estimators\n",
    "for res in cb_t.results:\n",
    "    print(res.best_score_)\n",
    "    print(res.best_estimator_)\n",
    "    \n",
    "    #This is the best estimator\n",
    "best_est = cb_t.results[0].best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_diag = OrangeBrick(true_labels=np.array(target),\n",
    "                      predicted_labels=best_est.predict_proba(cb_t.X)>0.5,\n",
    "                      var_names=target.columns).make_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=2,figsize=(10,10))\n",
    "\n",
    "cb_diag.confusion_chart(ax=ax[0])\n",
    "cb_diag.prec_rec_chart(ax=ax[1])\n",
    "\n",
    "#fig.suptitle('Model evaluation for GTR disciplines',y=1.01,size=16)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply model to the rest of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_has_text = cb.dropna(axis=0,subset=['long_description']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs_vect = cb_t.count_vect.transform(cb_has_text['long_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_has_text_pred = pd.DataFrame(best_est.predict_proba(all_docs_vect),columns=target.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add index\n",
    "cb_has_text_pred.index = cb_has_text['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_has_text.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_has_text_pred.to_csv(f'../../data/processed/{today_str}_predicted_sectors.csv',compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_has_text.set_index('id').to_csv(f'../../data/processed/{today_str}_predicted_metadata.csv',compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Journalism play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "out = pd.DataFrame(1-pairwise_distances(cb_has_text_pred.T,metric='cosine'),columns=cb_has_text_pred.columns,index=cb_has_text_pred.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_has_text.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
