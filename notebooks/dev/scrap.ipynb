{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset to focus on company entities.\n",
    "#There are some missing values in roles, which we would also drop (Note the False in the control flow)\n",
    "\n",
    "comps_cats = comps_cats.loc[['company' in x if type(x)!=NoneType else False for x in comps_cats['roles']]]\n",
    "\n",
    "comps_cats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify 'AI' companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find AI companies based on text description or the AI category\n",
    "\n",
    "#These are the terms we use for now. TODO: expand these using semantic similarities\n",
    "ai_terms = ['data science','machine learning', 'deep learning','artificial intelligence','neural network', ' ai ','natural language processing','text mining']\n",
    "\n",
    "#Lowercase the text\n",
    "comps_cats['long_description'] = comps_cats['long_description'].apply(lambda x: x.lower() if type(x)!=NoneType else np.nan)\n",
    "\n",
    "#Count the number of times that a company mentions AI \n",
    "comps_cats['ai_text_n'] = [sum([term in x for term in ai_terms]) if pd.isnull(x)==False else np.nan for x in comps_cats['long_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_cats['ai_text_n'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most companies that mention AI do this once. Others mention it more often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check a few companies with more than 3 AI mentions to see what they do\n",
    "for x in sample(list(comps_cats.loc[comps_cats['ai_text_n']>3]['long_description']),5):\n",
    "    print(x)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we check AI in categories. Note there might be other relevant categories in the data but we will not do this for now\n",
    "comps_cats['ai_cats'] = ['artificial intelligence' in c for c in comps_cats['category_name']]\n",
    "\n",
    "comps_cats['ai_cats'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the overlap between companies with AI categories and AI relateed text in the description?\n",
    "comp_cats_frequences = pd.crosstab(comps_cats['ai_text_n'],comps_cats['ai_cats'])\n",
    "\n",
    "#What is the distribution of companies that mention AI various times over the share of companies with AI in their category?\n",
    "comp_cats_frequences['text_share'] = 100*comp_cats_frequences[True]/comp_cats_frequences.sum(axis=1)\n",
    "\n",
    "comp_cats_frequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around a third of companies with AI in their categories don't mention AI related terms in their descriptions.\n",
    "There are quite a few companies that mention AI repeatedly but don't have an AI category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check of company descriptions for companies that have AI cats but no AI Terms\n",
    "\n",
    "#Check a few companies with more than 3 AI mentions to see what they do\n",
    "for x in sample(list(comps_cats.loc[(comps_cats['ai_text_n']==0)&(comps_cats['ai_cats']==True)]['long_description']),5):\n",
    "    print(x)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The companies that have ai categories but no ai related terms in their description look quite noisy. Let's exclude them from the analysis for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flag as AI companies with at least one AI term in their description. Later we could change this threshold\n",
    "comps_cats['ai_flag'] = comps_cats['ai_text_n']>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A couple of descriptive analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to create a year variable (founded on is currently a date)\n",
    " \n",
    "comps_cats['founded_year'] = [x.year if type(x)!=NoneType else np.nan for x in comps_cats['founded_on']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "(100*pd.crosstab(comps_cats['founded_year'],comps_cats['ai_flag'],normalize=1)).plot(ax=ax,title='Year share of activity')\n",
    "\n",
    "ax.set_xlim(2000,2018)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very interesting: explosion of AI startup activity while startup activity in general slows-up. What else could be explaining this? China's entry?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate country distribution\n",
    "country_distr = pd.crosstab(comps_cats['country'],comps_cats['ai_flag']).sort_values(True,ascending=False)\n",
    "\n",
    "country_distr[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the coverage issues are apparent - relatively limited activity in Japan. And where is China?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate index of comparative advantage\n",
    "country_distr['ai_rca']= (country_distr[True]/country_distr[True].sum())/(country_distr.sum(axis=1)/country_distr.sum(axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot RCAs for top 20 countries by level of activity\n",
    "(country_distr[:20]['ai_rca'].sort_values(ascending=False)-1).plot.bar(title='Relative specialisation in AI for top 20 countries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some results are expected (Israel, Singapore). Others (Canada), not so much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#That gnarly pivot gives us the number of ai companies per year and country.\n",
    "ai_country_counts = pd.pivot_table(comps_cats.groupby(['founded_year','country'])['ai_flag'].sum().reset_index(drop=False),index='country',columns='founded_year',values='ai_flag').fillna(0)\n",
    "\n",
    "#We want to focus our visualisation on the top 10 countries by overall activity\n",
    "bigger_countries = ai_country_counts.sum(axis=1).sort_values(ascending=False).index[:15]\n",
    "\n",
    "#Consider share of activity in a given year\n",
    "\n",
    "ai_country_shares = ai_country_counts.apply(lambda x: x/x.sum(),axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "ai_country_shares.loc[bigger_countries].T.rolling(window=3).mean().plot(ax=ax,title='Share of year in Country',figsize=(10,5),cmap='tab20',linewidth=2)\n",
    "\n",
    "ax.set_xlim(2000,2018)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone seems to be following a similar patterns perhaps with the exception of Singapore and Switzerland, which seem to be growing faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consider country sizes (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, product, chain\n",
    "import networkx as nx\n",
    "import community\n",
    "\n",
    "def flatten_list(a_list):\n",
    "    '''\n",
    "    Flattens a list\n",
    "    '''\n",
    "    \n",
    "    return([x for el in a_list for x in el])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(set(flatten_list(comps_cats['category_name'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here the idea is to create a proximity matrix based on co-occurrences\n",
    "\n",
    "#Turn co-occurrences into combinations of pairs we can use to construct a similarity matrix\n",
    "sector_combs = flatten_list([sorted(list(combinations(x,2))) for x in comps_cats['category_name']])\n",
    "sector_combs = [x for x in sector_combs if len(x)>0]\n",
    "\n",
    "#Turn the sector combs into an edgelist\n",
    "edge_list = pd.DataFrame(sector_combs,columns=['source','target'])\n",
    "\n",
    "edge_list['weight']=1\n",
    "\n",
    "#Group over edge pairs to aggregate weights\n",
    "edge_list_weighted = edge_list.groupby(['source','target'])['weight'].sum().reset_index(drop=False)\n",
    "\n",
    "edge_list_weighted.sort_values('weight',ascending=False).head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create network and extract communities\n",
    "net = nx.from_pandas_edgelist(edge_list_weighted,edge_attr=True)\n",
    "\n",
    "#We choose a high level of resulution (lower == more finely grained)\n",
    "comms = community.best_partition(net,resolution=0.1)\n",
    "\n",
    "#We have chosen quite a finely grained level here in order to obtain categories that we can label as creative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What does this look like?\n",
    "comm_strings = pd.DataFrame(comms,index=['comm']).T.groupby('comm')\n",
    "\n",
    "#This is just to visualise the participation in communities\n",
    "for n,x in enumerate(comm_strings.groups.keys()):\n",
    "    print(n)\n",
    "    print('====')\n",
    "    print('\\t'.join(list(comm_strings.groups[x])))\n",
    "    #print(', '.join(list(x.index())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creative sector lookup\n",
    "sector_labels = {0:'3d_printing',\n",
    "2:'advertising',\n",
    "5:'social_networks',\n",
    "7:'apps',\n",
    "9:'animation_film',\n",
    "10:'apps',\n",
    "12:'arts_culture',\n",
    "15:'audio_music',\n",
    "16:'immersive',\n",
    "24:'content_blogging',\n",
    "25:'advertising',\n",
    "45:'design_ux',\n",
    "53:'digital_media',\n",
    "52:'advertising',\n",
    "63:'video_games',\n",
    "64:'events_shows',\n",
    "67:'fashion',\n",
    "73:'photography',\n",
    "78:'smart_cities',\n",
    "88:'apps',\n",
    "89:'apps',\n",
    "92:'journalism_news',\n",
    "99:'design',\n",
    "111:'social_media',\n",
    "113:'video_editing',\n",
    "114:'web_design',\n",
    "116:'advertising',\n",
    "92:'e_books',\n",
    "11:'architecture'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that these categories may be too aggregate - eg sports games contain both video games (creative) and sports (non creative).\n",
    "\n",
    "# One way to deal with this is by increasing the granularity of the community detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lookup every category\n",
    "comps_cats['sector_list']= [[sector_labels[comms[lab]] if comms[lab] in sector_labels.keys() else 'not_creative' for lab in cats] for cats in comps_cats['category_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify one-sector categories\n",
    "comps_cats['sector_unique'] = [list(set(x))[0] if len(set(x))==1 else 'mixed' for x in comps_cats['sector_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looks like enough for a model\n",
    "#pd.Series(sect).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load lda_pipeline.py\n",
    "from gensim import corpora, models\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Characters to drop\n",
    "drop_characters = re.sub('-','',punctuation)+digits\n",
    "\n",
    "#Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('English')\n",
    "\n",
    "#Stem functions\n",
    "from nltk.stem import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def clean_tokenise(string,drop_characters=drop_characters,stopwords=stop):\n",
    "    '''\n",
    "    Takes a string and cleans (makes lowercase and removes stopwords)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "\n",
    "    #Lowercase\n",
    "    str_low = string.lower()\n",
    "    \n",
    "    \n",
    "    #Remove symbols and numbers\n",
    "    str_letters = re.sub('[{drop}]'.format(drop=drop_characters),'',str_low)\n",
    "    \n",
    "    \n",
    "    #Remove stopwords\n",
    "    clean = [x for x in str_letters.split(' ') if (x not in stop) & (x!='')]\n",
    "    \n",
    "    return(clean)\n",
    "\n",
    "\n",
    "class CleanTokenize():\n",
    "    '''\n",
    "    This class takes a list of strings and returns a tokenised, clean list of token lists ready\n",
    "    to be processed with the LdaPipeline\n",
    "    \n",
    "    It has a clean method to remove symbols and stopwords\n",
    "    \n",
    "    It has a bigram method to detect collocated words\n",
    "    \n",
    "    It has a stem method to stem words\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,corpus):\n",
    "        '''\n",
    "        Takes a corpus (list where each element is a string)\n",
    "        '''\n",
    "        \n",
    "        #Store\n",
    "        self.corpus = corpus\n",
    "        \n",
    "    def clean(self,drop=drop_characters,stopwords=stop):\n",
    "        '''\n",
    "        Removes strings and stopwords, \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        cleaned = [clean_tokenise(doc,drop_characters=drop,stopwords=stop) for doc in self.corpus]\n",
    "        \n",
    "        self.tokenised = cleaned\n",
    "        return(self)\n",
    "    \n",
    "    def stem(self):\n",
    "        '''\n",
    "        Optional: stems words\n",
    "        \n",
    "        '''\n",
    "        #Stems each word in each tokenised sentence\n",
    "        stemmed = [[stemmer.stem(word) for word in sentence] for sentence in self.tokenised]\n",
    "    \n",
    "        self.tokenised = stemmed\n",
    "        return(self)\n",
    "        \n",
    "    \n",
    "    def bigram(self,threshold=10):\n",
    "        '''\n",
    "        Optional Create bigrams.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Colocation detector trained on the data\n",
    "        phrases = models.Phrases(self.tokenised,threshold=threshold)\n",
    "        \n",
    "        bigram = models.phrases.Phraser(phrases)\n",
    "        \n",
    "        self.tokenised = bigram[self.tokenised]\n",
    "        \n",
    "        return(self)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "class LdaPipeline():\n",
    "    '''\n",
    "    This class processes lists of keywords.\n",
    "    How does it work?\n",
    "    -It is initialised with a list where every element is a collection of keywords\n",
    "    -It has a method to filter keywords removing those that appear less than a set number of times\n",
    "    \n",
    "    -It has a method to process the filtered df into an object that gensim can work with\n",
    "    -It has a method to train the LDA model with the right parameters\n",
    "    -It has a method to predict the topics in a corpus\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,corpus):\n",
    "        '''\n",
    "        Takes the list of terms\n",
    "        '''\n",
    "        \n",
    "        #Store the corpus\n",
    "        self.tokenised = corpus\n",
    "        \n",
    "    def filter(self,minimum=5):\n",
    "        '''\n",
    "        Removes keywords that appear less than 5 times.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load\n",
    "        tokenised = self.tokenised\n",
    "        \n",
    "        #Count tokens\n",
    "        token_counts = pd.Series([x for el in tokenised for x in el]).value_counts()\n",
    "        \n",
    "        #Tokens to keep\n",
    "        keep = token_counts.index[token_counts>minimum]\n",
    "        \n",
    "        #Filter\n",
    "        tokenised_filtered = [[x for x in el if x in keep] for el in tokenised]\n",
    "        \n",
    "        #Store\n",
    "        self.tokenised = tokenised_filtered\n",
    "        self.empty_groups = np.sum([len(x)==0 for x in tokenised_filtered])\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def clean(self):\n",
    "        '''\n",
    "        Remove symbols and numbers\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    def process(self):\n",
    "        '''\n",
    "        This creates the bag of words we use in the gensim analysis\n",
    "        \n",
    "        '''\n",
    "        #Load the list of keywords\n",
    "        tokenised = self.tokenised\n",
    "        \n",
    "        #Create the dictionary\n",
    "        dictionary = corpora.Dictionary(tokenised)\n",
    "        \n",
    "        #Create the Bag of words. This converts keywords into ids\n",
    "        corpus = [dictionary.doc2bow(x) for x in tokenised]\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.dictionary = dictionary\n",
    "        return(self)\n",
    "        \n",
    "    def tfidf(self):\n",
    "        '''\n",
    "        This is optional: We extract the term-frequency inverse document frequency of the words in\n",
    "        the corpus. The idea is to identify those keywords that are more salient in a document by normalising over\n",
    "        their frequency in the whole corpus\n",
    "        \n",
    "        '''\n",
    "        #Load the corpus\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Fit a TFIDF model on the data\n",
    "        tfidf = models.TfidfModel(corpus)\n",
    "        \n",
    "        #Transform the corpus and save it\n",
    "        self.corpus = tfidf[corpus]\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def fit_lda(self,num_topics=20,passes=5,iterations=75,random_state=1803):\n",
    "        '''\n",
    "        \n",
    "        This fits the LDA model taking a set of keyword arguments.\n",
    "        #Number of passes, iterations and random state for reproducibility. We will have to consider\n",
    "        reproducibility eventually.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load the corpus\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Train the LDA model with the parameters we supplied\n",
    "        lda = models.LdaModel(corpus,id2word=self.dictionary,\n",
    "                              num_topics=num_topics,passes=passes,iterations=iterations,random_state=random_state)\n",
    "        \n",
    "        #Save the outputs\n",
    "        self.lda_model = lda\n",
    "        self.lda_topics = lda.show_topics(num_topics=num_topics)\n",
    "        \n",
    "\n",
    "        return(self)\n",
    "    \n",
    "    def predict_topics(self):\n",
    "        '''\n",
    "        This predicts the topic mix for every observation in the corpus\n",
    "        \n",
    "        '''\n",
    "        #Load the attributes we will be working with\n",
    "        lda = self.lda_model\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Now we create a df\n",
    "        predicted = lda[corpus]\n",
    "        \n",
    "        #Convert this into a dataframe\n",
    "        predicted_df = pd.concat([pd.DataFrame({x[0]:x[1] for x in topics},\n",
    "                                              index=[num]) for num,topics in enumerate(predicted)]).fillna(0)\n",
    "        \n",
    "        self.predicted_df = predicted_df\n",
    "        \n",
    "        return(self)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load text_classifier.py\n",
    "# CLasses\n",
    "\n",
    "#ML imports\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score,GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore',UserWarning)\n",
    "\n",
    "#One class for text classification based on text inputs\n",
    "\n",
    "class TextClassification():\n",
    "    '''\n",
    "    This class takes a corpus (could be a list of strings or a tokenised corpus) and a target (could be multiclass or single class).\n",
    "    \n",
    "    When it is initialised it vectorises the list of tokens using sklearn's count vectoriser.\n",
    "    \n",
    "    It has a grid search method that takes a list of models and parameters and trains the model.\n",
    "    \n",
    "    It returns the output of grid search for diagnosis\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,corpus,target):\n",
    "        '''\n",
    "        \n",
    "        Initialise. The class will recognise if we are feeding it a list of strings or a list of\n",
    "        tokenised documents and vectorise accordingly. \n",
    "        \n",
    "        It will also recognise is this a multiclass or one class problem based on the dimensions of the target array\n",
    "        \n",
    "        Later on, it will use control flow to modify model parameters depending on the type of data we have\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Is this a multiclass classification problem or a single class classification problem?\n",
    "        if target.shape[1]>1:\n",
    "            self.mode = 'multiclass'\n",
    "            \n",
    "        else:\n",
    "            self.mode = 'single_class'\n",
    "    \n",
    "    \n",
    "        #Store the target\n",
    "        self.Y = target\n",
    "    \n",
    "        #Did we feed the model a bunch of strings or a list of tokenised docs? If the latter, we clean and tokenise.\n",
    "        \n",
    "        if type(corpus[0])==str:\n",
    "            corpus = CleanTokenize(corpus).clean().bigram().tokenised\n",
    "            \n",
    "        #Turn every list of tokens into a string for count vectorising\n",
    "        corpus_string =  [' '.join(words) for words in corpus]\n",
    "        \n",
    "        \n",
    "        #And then we count vectorise in a hacky way.\n",
    "        count_vect = CountVectorizer(stop_words='english',min_df=5,ngram_range=[1,2]).fit(corpus_string)\n",
    "        \n",
    "        #Store the features\n",
    "        self.X = count_vect.transform(corpus_string)\n",
    "        \n",
    "        #Store the count vectoriser (we will use it later on for prediction on new data)\n",
    "        self.count_vect = count_vect\n",
    "        \n",
    "    def grid_search(self,models):\n",
    "        '''\n",
    "        The grid search method takes a list with models and their parameters and it does grid search crossvalidation.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load inputs and targets into the model\n",
    "        Y = self.Y\n",
    "        X = self.X\n",
    "        \n",
    "        if self.mode=='multiclass':\n",
    "            '''\n",
    "            If the model is multiclass then we need to add some prefixes to the model paramas\n",
    "            \n",
    "            '''\n",
    "        \n",
    "            for mod in models:\n",
    "                #Make ovr\n",
    "                mod[0] = OneVsRestClassifier(mod[0])\n",
    "                \n",
    "                #Add the estimator prefix\n",
    "                mod[1] = {'estimator__'+k:v for k,v in mod[1].items()}\n",
    "                \n",
    "        \n",
    "        #Container with results\n",
    "        results = []\n",
    "\n",
    "        #For each model, run the analysis.\n",
    "        for num,mod in enumerate(models):\n",
    "            print(num)\n",
    "\n",
    "            #Run the classifier\n",
    "            clf = GridSearchCV(mod[0],mod[1])\n",
    "\n",
    "            #Fit\n",
    "            clf.fit(X,Y)\n",
    "\n",
    "            #Append results\n",
    "            results.append(clf)\n",
    "        \n",
    "        self.results = results\n",
    "        return(self)\n",
    "\n",
    "    \n",
    "#Class to visualise the outputs of multilabel models.\n",
    "\n",
    "#I call it OrangeBrick after YellowBrick, the package for ML output visualisation \n",
    "#(which currently doesn't support multilabel classification)\n",
    "\n",
    "\n",
    "class OrangeBrick():\n",
    "    '''\n",
    "    This class takes a df with the true classes for a multilabel classification exercise and produces some charts visualising findings.\n",
    "    \n",
    "    The methods include:\n",
    "    \n",
    "        .confusion_stack: creates a stacked barchart with the confusion matrices stacked by category, sorting classes by performance\n",
    "        .prec_rec: creates a barchart showing each class precision and recall;\n",
    "        #Tobe done: Consider mixes between classes?\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,true_labels,predicted_labels,var_names):\n",
    "        '''\n",
    "        Initialise with a true labels, predicted labels and the variable names\n",
    "        '''\n",
    "         \n",
    "        self.true_labels = true_labels\n",
    "        self.predicted_labels = predicted_labels\n",
    "        self.var_names = var_names\n",
    "    \n",
    "    def make_metrics(self):\n",
    "        '''\n",
    "        Estimates performance metrics (for now just confusion charts by class and precision/recall scores for the 0.5 \n",
    "        decision rule.\n",
    "        \n",
    "        '''\n",
    "        #NB in a confusion matrix in SKlearn the X axis indicates the predicted class and the Y axis indicates the ground truth.\n",
    "        #This means that:\n",
    "            #cf[0,0]-> TN\n",
    "            #cf[1,1]-> TP\n",
    "            #cf[0,1]-> FN (prediction is false, groundtruth is true)\n",
    "            #cf[1,0]-> FP (prediction is true, ground truth is false)\n",
    "\n",
    "\n",
    "\n",
    "        #Predictions and true labels\n",
    "        true_labels = self.true_labels\n",
    "        pred_labels = self.predicted_labels\n",
    "\n",
    "        #Variable names\n",
    "        var_names = self.var_names\n",
    "\n",
    "        #Store confusion matrices\n",
    "        score_store = []\n",
    "\n",
    "\n",
    "        for num in np.arange(len(var_names)):\n",
    "\n",
    "            #This is the confusion matrix\n",
    "            cf = confusion_matrix(pred_labels[:,num],true_labels[:,num])\n",
    "\n",
    "            #This is a melted confusion matrix\n",
    "            melt_cf = pd.melt(pd.DataFrame(cf).reset_index(drop=False),id_vars='index')['value']\n",
    "            melt_cf.index = ['true_negative','false_positive','false_negative','true_positive']\n",
    "            melt_cf.name = var_names[num]\n",
    "            \n",
    "            #Order variables to separate failed vs correct predictions\n",
    "            melt_cf = melt_cf.loc[['true_positive','true_negative','false_positive','false_negative']]\n",
    "\n",
    "            #We are also interested in precision and recall\n",
    "            prec = cf[1,1]/(cf[1,1]+cf[1,0])\n",
    "            rec = cf[1,1]/(cf[1,1]+cf[0,1])\n",
    "\n",
    "            prec_rec = pd.Series([prec,rec],index=['precision','recall'])\n",
    "            prec_rec.name = var_names[num]\n",
    "            score_store.append([melt_cf,prec_rec])\n",
    "    \n",
    "        self.score_store = score_store\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def confusion_chart(self,ax):\n",
    "        '''\n",
    "        Plot the confusion charts\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Visualise confusion matrix outputs\n",
    "        cf_df = pd.concat([x[0] for x in self.score_store],1)\n",
    "\n",
    "        #This ranks categories by the error rates\n",
    "        failure_rate = cf_df.apply(lambda x: x/x.sum(),axis=0).loc[['false' in x for x in cf_df.index]].sum().sort_values(\n",
    "            ascending=False).index\n",
    "\n",
    "        \n",
    "        #Plot and add labels\n",
    "        cf_df.T.loc[failure_rate,:].plot.bar(stacked=True,ax=ax,width=0.8,cmap='Accent')\n",
    "\n",
    "        ax.legend(bbox_to_anchor=(1.01,1))\n",
    "        #ax.set_title('Stacked confusion matrix for disease areas',size=16)\n",
    "    \n",
    "    \n",
    "    def prec_rec_chart(self,ax):\n",
    "        '''\n",
    "        \n",
    "        Plot a precision-recall chart\n",
    "        \n",
    "        '''\n",
    "    \n",
    "\n",
    "        #Again, we sort them here to assess model performance in different disease areas\n",
    "        prec_rec = pd.concat([x[1] for x in self.score_store],1).T.sort_values('precision')\n",
    "        prec_rec.plot.bar(ax=ax)\n",
    "\n",
    "        #Add legend and title\n",
    "        ax.legend(bbox_to_anchor=(1.01,1))\n",
    "        #ax.set_title('Precision and Recall by disease area',size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is the corpus. We drop projects without any org labels, and projects without abstracts\n",
    "\n",
    "#We focus on 'pure cases'\n",
    "cb_pure_cases = comps_cats.loc[[x not in ['mixed','not_creative'] for x in comps_cats['sector_unique']]].reset_index(drop=True)\n",
    "\n",
    "#Drop cases with no descriptions\n",
    "cb_pure_cases = cb_pure_cases.dropna(axis=0,subset=['long_description']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cb_pure_cases['sector_unique'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(cb_pure_cases['long_description'])\n",
    "\n",
    "#We use a utility function to create a df for a one vs rest classification\n",
    "target = pd.get_dummies(cb_pure_cases['sector_unique'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run grid search with these model parameters\n",
    "my_models = [\n",
    "    [RandomForestClassifier(),\n",
    "     {'class_weight':['balanced'],'min_samples_leaf':[1,5]}],\n",
    "    \n",
    "    [LogisticRegression(),\n",
    "     {'class_weight':['balanced'],'penalty':['l1','l2'],\n",
    "      'C':[0.1,1,100]}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict groups\n",
    "\n",
    "#Initialise the TextClassification class\n",
    "cb_t = TextClassification(corpus,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_t.grid_search(my_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check scores and best estimators\n",
    "for res in cb_t.results:\n",
    "    print(res.best_score_)\n",
    "    print(res.best_estimator_)\n",
    "    \n",
    "    #This is the best estimator\n",
    "best_est = cb_t.results[1].best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_diag = OrangeBrick(true_labels=np.array(target),\n",
    "                      predicted_labels=best_est.predict_proba(cb_t.X)>0.5,\n",
    "                      var_names=target.columns).make_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=2,figsize=(10,10))\n",
    "\n",
    "cb_diag.confusion_chart(ax=ax[0])\n",
    "cb_diag.prec_rec_chart(ax=ax[1])\n",
    "\n",
    "#fig.suptitle('Model evaluation for GTR disciplines',y=1.01,size=16)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the model to arXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arx = pd.read_csv('../data/processed/6_8_2019_arxiv_processed.csv',compression='zip')\n",
    "\n",
    "# arx_papers = arx.drop_duplicates('article_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the arXiv data using the same model we used to train the model before\n",
    "\n",
    "# arx_trans = cb_t.count_vect.transform(arx_papers['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arx_preds = pd.DataFrame(best_est.predict_proba(arx_trans),columns=target.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for abs in arx_papers.loc[arx_preds['video_games']>0.99999]['summary'][:10]:\n",
    "    \n",
    "#     print(abs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alas, it doesn't work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse AI trends in creative sectors in CrunchBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all observations without description\n",
    "cb_with_descr = comps_cats.dropna(axis=0,subset=['long_description']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the corpus\n",
    "all_cb_transformed = cb_t.count_vect.transform(cb_with_descr['long_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate predictions\n",
    "all_preds = pd.DataFrame(\n",
    "    best_est.predict_proba(all_cb_transformed)>0.9999,\n",
    "    columns=target.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can we use the predicted labels with the rest of the CrunchBase data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_with_descr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cb_descr= pd.concat([cb_with_descr[['id','long_description']],all_preds],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for s in out.columns:\n",
    "    \n",
    "#     print(s)\n",
    "#     print('=====')\n",
    "    \n",
    "#     print('\\n')\n",
    "#     for x in all_cb_descr.loc[all_cb_descr[s]==True,'long_description'][:5]:\n",
    "        \n",
    "#         print(x[:500])\n",
    "        \n",
    "#         print('\\n')\n",
    "    \n",
    "#     print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above looks fine. We will reclassify these companies into creative sectors and calculate some descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_sector_lookup = {\n",
    "    '3d_printing':'crafts', \n",
    "    'advertising':'advertising',\n",
    "    'animation_film':'film_video_tv',\n",
    "    'apps':'software',\n",
    "    'arts_culture':'music_performing_arts',\n",
    "    'audio_music':'music_performing_arts',\n",
    "    'content_blogging':'publishing',\n",
    "    'design':'design',\n",
    "    'design_ux':'design',\n",
    "    'e_books':'publishing',\n",
    "    #'events_shows':'music_performing_arts',\n",
    "    #'fashion':'design',\n",
    "    'immersive':'games_immersive',\n",
    "    'journalism_news':'publishing',\n",
    "    'photography':'film_video_tv',\n",
    "    'social_media':'software',\n",
    "    'social_networks':'software',\n",
    "    'video_editing':'film_video_tv',\n",
    "    'video_games':'games_immersive',\n",
    "    'web_design':'design',\n",
    "    'architecture':'architecture',\n",
    "    'digital_media':'film_video_tv'\n",
    "}\n",
    "\n",
    "\n",
    "#Turn into a df for merging\n",
    "creative_df = pd.DataFrame.from_dict(creative_sector_lookup,orient='index',\n",
    "                                    columns=['sector']).reset_index(drop=False)\n",
    "creative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to convert the predicted data across categories. Do Melt, apply, pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_sector_merged = pd.merge(pd.melt(all_cb_descr,id_vars=['id','long_description']).reset_index(drop=False),\n",
    "                 creative_df,left_on='variable',right_on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_sector_merged['value'] = cb_sector_merged['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the missing values\n",
    "cb_sector_reshaped = cb_sector_merged.loc[cb_sector_merged['value']==True].pivot_table(\n",
    "    index=['id'],columns='sector',values='value',aggfunc=sum).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cb_sector_reshaped>0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we merge cb sector reshaped with the cb df with all sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_sectors = pd.merge(cb_with_descr,cb_sector_reshaped.reset_index(drop=False),\n",
    "                     left_on='id',right_on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive analysis of AI CrunchBase activity globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig(figname,path='../reports/figures/figures_report/cb_'):\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(path+figname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*len(cb_sectors)/len(cb_with_descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*cb_sectors['ai_flag'].sum()/len(cb_sectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*cb_with_descr['ai_flag'].sum()/len(cb_with_descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cb_with_descr['ai_flag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sectoral distribution of activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_sectors = set(creative_sector_lookup.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_distr = pd.concat([cb_sectors.loc[cb_sectors[sector]>0]['ai_flag'].value_counts() for sector in creative_sectors],axis=1)\n",
    "sector_distr.columns = creative_sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_distr.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(100*sector_distr.T.apply(lambda x: x/x.sum(),axis=1).sort_values(\n",
    "    True,ascending=False)[True]).plot.bar(title='Share of AI projects in vertical')\n",
    "\n",
    "save_fig('ai_share_sector.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three examples from each sector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in creative_sectors:\n",
    "    \n",
    "    print(s)\n",
    "    print('===')\n",
    "    \n",
    "    ai_ex = cb_sectors.loc[(cb_sectors[s]>0) & (cb_sectors['ai_flag']==True)]\n",
    "    \n",
    "    choose_three = random.sample(list(ai_ex['long_description']),3)\n",
    "    \n",
    "    for des in choose_three:\n",
    "        \n",
    "        print(des[:500])\n",
    "        print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sectoral distribution of funding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_fund_distr = pd.concat([cb_sectors.loc[cb_sectors[sector]>0].groupby(\n",
    "    'ai_flag')['funding_total_usd'].sum() for sector in creative_sectors],axis=1)\n",
    "sector_fund_distr.columns = creative_sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_fund_distr.T.sort_values(True,ascending=False)/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sectors = sector_fund_distr.T.sort_values(True,ascending=False)[True].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overal trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_sectors['founded_on'][0].year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_sectors['year_founded'] = [x.year for x in cb_sectors['founded_on']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_year = pd.concat([cb_sectors.loc[(cb_sectors[sect]>0)&(cb_sectors['ai_flag']==True)][\n",
    "    'year_founded'].value_counts() for sect in creative_sectors],axis=1).fillna(0)\n",
    "                               \n",
    "sector_year = sector_year.loc[(sector_year.index>2000)&(sector_year.index<2019)]\n",
    "\n",
    "sector_year.columns = creative_sectors\n",
    "\n",
    "sector_year[top_sectors].plot.bar(stacked=True,title='Number of companies')\n",
    "\n",
    "save_fig('ai_sector_trends.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! There has been a drop in the number of companies in the last couple of years? What about levels of funding?\n",
    "\n",
    "Let's look at the comparative picture first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_year_all = cb_with_descr['founded_year'].value_counts()\n",
    "\n",
    "cb_year_rec = cb_year_all.loc[np.arange(2000,2019)]\n",
    "\n",
    "cb_year_rec.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_year_all_ai = cb_with_descr.loc[cb_with_descr['ai_flag']==True,'founded_year'].value_counts()\n",
    "cb_year_rec_ai = cb_year_all_ai.loc[np.arange(2000,2019)]\n",
    "\n",
    "cb_year_rec_ai.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors_sorted = sector_year.sum().sort_values().index[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sector_year.apply(lambda x: x/x.sum(),axis=1)[sectors_sorted].rolling(window=3).mean().dropna().plot.bar(\n",
    "    stacked=True,title='share of AI activity',figsize=(8,5))\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "save_fig('ai_sector_share_trends.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_year_funding = pd.concat([cb_sectors.loc[(cb_sectors[sect]>0)&(cb_sectors['ai_flag']==True)].groupby(\n",
    "    'year_founded')['funding_total_usd'].sum() for sect in creative_sectors],axis=1).fillna(0)\n",
    "                               \n",
    "sector_year_funding = sector_year_funding.loc[(sector_year_funding.index>2000)&(sector_year_funding.index<2019)]\n",
    "\n",
    "sector_year_funding.columns = creative_sectors\n",
    "\n",
    "ax = sector_year_funding.rolling(window=2).mean().plot.bar(stacked=True,title='Level of funding for creative AI',\n",
    "                                                          figsize=(8,5))\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "save_fig('ai_sector_trends_funding.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What companies received the biggest amount of funding per sector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in creative_sectors:\n",
    "    \n",
    "    print(s)\n",
    "    print('===')\n",
    "    \n",
    "    ai_ex = cb_sectors.loc[(cb_sectors[s]>0) & (cb_sectors['ai_flag']==True)]\n",
    "    \n",
    "    top_3 = ai_ex.sort_values('funding_total_usd',ascending=False)[:3]\n",
    "    \n",
    "    for pid,des in top_3.iterrows():\n",
    "        \n",
    "        print(des['company_name'])\n",
    "        print(des['year_founded'])\n",
    "        print('$'+str(des['funding_total_usd']/1e6)+' million')\n",
    "        print('----')\n",
    "        \n",
    "        print(des['long_description'][:500])\n",
    "                                  \n",
    "        print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lq_df(df):\n",
    "    '''\n",
    "    Takes a df with cells = activity in col in row and returns a df with cells = lq\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    area_activity = df.sum(axis=0)\n",
    "    area_shares = area_activity/area_activity.sum()\n",
    "    \n",
    "    lqs = df.apply(lambda x: (x/x.sum())/area_shares, axis=1)\n",
    "    return(lqs)\n",
    "\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_country = pd.concat([cb_sectors.loc[(cb_sectors[sect]>0)&(cb_sectors['ai_flag']==True)][\n",
    "    'country'].value_counts() for sect in creative_sectors],axis=1).fillna(0)\n",
    "                               \n",
    "sector_country.columns = creative_sectors\n",
    "\n",
    "big_countries = sector_country.sum(axis=1).sort_values(ascending=False).index[:15]\n",
    "\n",
    "sector_country.loc[big_countries,top_sectors].plot.bar(stacked=True)\n",
    "\n",
    "save_fig('ai_country_totals.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*len(cb_sectors.loc[(cb_sectors['country']=='United States')&(cb_sectors['ai_flag'])==True])/len(cb_sectors.loc[cb_sectors['ai_flag']==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*len(cb_sectors.loc[(cb_sectors['country']=='United Kingdom')&(cb_sectors['ai_flag'])==True])/len(cb_sectors.loc[cb_sectors['ai_flag']==True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Market 'shares'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sector_country.apply(lambda x: x/x.sum(),axis=0).loc[big_countries,top_sectors].T.plot.bar(\n",
    "    stacked=True,cmap='tab20',edgecolor='lightgrey',title='Market shares',figsize=(10,5))\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "save_fig('ai_country_shares.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specialisation (discretised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to extract non creative AI companies and non AI companies\n",
    "#Non creative AI\n",
    "ai_creative_ids = set(cb_sectors.loc[cb_sectors['ai_flag']==True]['id'])\n",
    "non_ci_ai = cb_with_descr.loc[[x not in ai_creative_ids for x in cb_with_descr['id']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_ai_country = pd.DataFrame(non_ci_ai.loc[non_ci_ai['ai_flag']==True]['country'].value_counts())\n",
    "\n",
    "non_ai_country.columns= ['AI non CI']\n",
    "\n",
    "non_ai_all = cb_with_descr.loc[cb_with_descr['ai_flag']==False]['country'].value_counts()\n",
    "\n",
    "non_ai_all.name = 'Non AI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_country_activity = pd.concat([non_ai_all,non_ai_country,sector_country[top_sectors]],axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_country_lq = create_lq_df(all_country_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "ax = sn.heatmap(all_country_lq.apply(lambda x: pd.qcut(x,np.arange(0,1.1,0.2),labels=False,duplicates='drop'),axis=1).loc[big_countries].T,\n",
    "           cmap='Oranges',edgecolor='lighgrey',linewidth=0.01,ax=ax)\n",
    "\n",
    "ax.collections[0].colorbar.set_label(\"Specialisation quartile\")\n",
    "ax.set_title('Country specialisation')\n",
    "\n",
    "save_fig('ai_sector_specs.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution of market shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sector_year_trend(df,year_var,geo_var):\n",
    "    '''\n",
    "    Creates a table with number of papers by country and year\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    country_year = pd.crosstab(df[year_var],df[geo_var])\n",
    "    \n",
    "    return(country_year)\n",
    "\n",
    "\n",
    "def geo_trends_country(df,ai_var,year_var,geo_var,sector_names,threshold,country,top_c=False,year_lims= False):\n",
    "    \n",
    "    '''\n",
    "    Visualise geotrends by sector\n",
    "    \n",
    "    Creates a set of tables for each sector with number of papers by country and year\n",
    "    \n",
    "    args:\n",
    "    \n",
    "        -df is the table with the information, every row is an entity with geographical information, ai information, sector information etc\n",
    "        -ai_var is the ai variable\n",
    "        -geo_var is the variable with the countries\n",
    "        -sector_names is the sectors we want to extract information for\n",
    "        -threshold is the threshold above which we accept a paper as 'creative'\n",
    "        -top_c is the number of countries we want to output\n",
    "        \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    #We focus on ai papers\n",
    "    \n",
    "\n",
    "    \n",
    "    ai_papers = df.loc[df[ai_var]==True]\n",
    "    \n",
    "    #We store the sectoral results here\n",
    "    sector_store = {}\n",
    "    \n",
    "    #Also for all papers (as a benchmark)\n",
    "    \n",
    "    all_papers = make_sector_year_trend(df,year_var,geo_var)\n",
    "    \n",
    "    all_papers_shares = all_papers.apply(lambda x: x/x.sum(),axis=1)\n",
    "    \n",
    "    sector_store['All'] = all_papers_shares[country]\n",
    "    \n",
    "    #For each sector\n",
    "    for s in sector_names:\n",
    "        \n",
    "        \n",
    "        #Calculate number of papers in sector by year and country\n",
    "        \n",
    "        out = make_sector_year_trend(ai_papers.loc[ai_papers[s]>threshold],year_var,geo_var).fillna(0)\n",
    "        \n",
    "        \n",
    "        out_shares = out.apply(lambda x: x/x.sum(),axis=1)\n",
    "        \n",
    "        #This is to limit the number of years we focus on\n",
    "        if year_lims!=False:\n",
    "            \n",
    "            out_shares = out_shares.loc[(out_shares.index>year_lims[0]) & (out_shares.index<year_lims[1])]\n",
    "        \n",
    "        \n",
    "        #out['sector']=s\n",
    "        try:\n",
    "            sector_store[s]=out_shares[country]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return(sector_store)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_all = []\n",
    "\n",
    "out_uk = []\n",
    "\n",
    "for s in sector_year:\n",
    "    \n",
    "    s_out = cb_sectors.loc[cb_sectors[s]>0]\n",
    "    \n",
    "    country_by_year = pd.crosstab(s_out['year_founded'],s_out['country'])\n",
    "    \n",
    "    \n",
    "    if 'United Kingdom' in country_by_year.columns:\n",
    "        out_uk.append(country_by_year['United Kingdom'])\n",
    "    out_all.append(country_by_year)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_uk_creative = pd.concat(out_uk,axis=1).fillna(0).sum(axis=1).loc[np.arange(2000,2019)]\n",
    "ai_all_creative = pd.concat(out_all,axis=1).fillna(0).sum(axis=1).loc[np.arange(2000,2019)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (100*(ai_uk_creative/ai_all_creative).rolling(window=3).mean().dropna()).plot(title='UK Market share in creative AI activity')\n",
    "\n",
    "ax.set_ylim(0,10)\n",
    "\n",
    "save_fig('uk_market_share.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in big_countries:\n",
    "    \n",
    "    print(c)\n",
    "    print('===')\n",
    "    \n",
    "    ai_ex = cb_sectors.loc[(cb_sectors['ai_flag']==True)&(cb_sectors['country']==c)]\n",
    "    \n",
    "    top_3 = ai_ex.sort_values('funding_total_usd',ascending=False)[:10]\n",
    "    \n",
    "    for pid,des in top_3.iterrows():\n",
    "        \n",
    "        print(des['company_name'])\n",
    "        print(des['year_founded'])\n",
    "        print('$'+str(des['funding_total_usd']/1e6)+' million')\n",
    "        print('----')\n",
    "        \n",
    "        print(des['long_description'][:500])\n",
    "                                  \n",
    "        print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output keyword lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_appendix_lookup = {k:sector_labels[v] for k,v in comms.items() if v in sector_labels.keys()}\n",
    "creative_appendix_lookup_2 = {k:creative_sector_lookup[v] for k,v in creative_appendix_lookup.items() if v in creative_sector_lookup.keys()}\n",
    "\n",
    "append = pd.DataFrame(creative_appendix_lookup_2,index=['keyword']).T.reset_index(drop=False)\n",
    "append.groupby('keyword')['index'].apply(lambda x: ', '.join(x)).to_csv('../data/external/cb_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_sectors.loc[cb_sectors['country']=='United Kingdom'].to_csv(f'../data/processed/{today_str}_uk_creative_companies.csv',compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_sectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/processed/{today_str}_arxiv_creative_sector_names.json','w') as outfile:\n",
    "    json.dump(list(top_sectors),outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_getters.labs.core import download_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
